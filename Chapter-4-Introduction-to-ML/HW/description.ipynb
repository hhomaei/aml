{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d814687e",
   "metadata": {},
   "source": [
    "<h1 align='center'>Homework 3<br>KNN Implementation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df9537",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "In this homework, you will implement three custom K-Nearest Neighbors (KNN) classifiers that handle numeric, categorical, and mixed features without preprocessing. You should use separate distance metrics for each feature type and combine them in a weighted fashion to classify new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6f49b",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "Students will implement K-Nearest Neighbors (KNN) classifiers tailored to the feature types in three datasets:\n",
    "- Numeric-only features\n",
    "- Categorical-only features\n",
    "- Mixed (categorical and numeric) features\n",
    "\n",
    "Each part includes a concrete dataset that can be used as-is, with no preprocessing required.\n",
    "\n",
    "This assignment helps you understand how distance metrics affect classification when feature types vary. You will also see how the number of neighbors affects the prediction results. You will be asked to implement KNN yourself and compare the results of your own implementation with the `KNeighborsClassifier` from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f146ce9",
   "metadata": {},
   "source": [
    "## Part 1: Numeric-only classification (Breast Cancer dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3f09a",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Dataset: Breast Cancer Wisconsin (Diagnostic) dataset.\n",
    "- Features: 30 numerical measurements of cell nuclei.\n",
    "- Target: malignant vs. benign.\n",
    "- Task: Implement KNN using Euclidean distance, evaluate accuracy across different values of $K$, and plot accuracy vs. $K$. Then, compare the results with scikit-learn’s built-in implementation of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23d359",
   "metadata": {},
   "source": [
    "### Step-by-step implementation guide\n",
    "1. Load the dataset from `sklearn.datasets` using `load_breast_cancer()` function.\n",
    "2. Split into training and test sets.\n",
    "3. Implement Euclidean distance manually.\n",
    "4. Write your own KNN classifier using that distance.\n",
    "5. Evaluate accuracy for different values of $K$.\n",
    "6. Plot accuracy vs. $K$.\n",
    "7. Compare results with scikit-learn’s ```KNeighborsClassifier```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91df83",
   "metadata": {},
   "source": [
    "### Sample result\n",
    "\n",
    "<img src=\"images/breast-cancer-KNN-manual-vs-builtin.png\" width=\"40%\" height=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a9446",
   "metadata": {},
   "source": [
    "## Part 2: Categorical-only classification (Car Evaluation dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f272a5",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Dataset: UCI Car Evaluation dataset.\n",
    "- Features: buying, maint, doors, persons, lug_boot, safety (all categorical).\n",
    "- Target: car acceptability.\n",
    "- Task: Implement KNN using Hamming distance, evaluate accuracy across different values of $K$, and plot accuracy vs. $K$. Then, compare the results with scikit-learn’s built-in implementation of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c66363",
   "metadata": {},
   "source": [
    "### Step-by-step implementation guide\n",
    "1. Load the dataset directly from [UCI Website](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data \"Car Evaluation dataset\").\n",
    "2. Split into training and test sets.\n",
    "3. Implement Hamming distance manually.\n",
    "4. Write your own KNN classifier using that distance.\n",
    "5. Evaluate accuracy for different values of $K$.\n",
    "6. Plot accuracy vs. $K$.\n",
    "7. Compare results with scikit-learn’s ```KNeighborsClassifier``` (using metric=\"hamming\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c1c4d",
   "metadata": {},
   "source": [
    "### Sample result\n",
    "\n",
    "<img src=\"images/car-evaluation-KNN-manual-vs-builtin.png\" width=\"40%\" height=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145bfdf",
   "metadata": {},
   "source": [
    "## Part 3: Mixed-feature classification (Restaurant Tips dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4d215",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Dataset: Seaborn Restaurant Tips dataset.\n",
    "- Features: Mix of categorical (sex, smoker, day) and numeric (total_bill, tip, size).\n",
    "- Target: Time (Lunch/Dinner).\n",
    "- Task: Implement a mixed-distance KNN. Use Euclidean distance for numeric features, Hamming distance for categorical features, combine them with a weight $W$, evaluate across different $K$ and $W$, and plot accuracy vs. $K$ for multiple $W$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291dfc6",
   "metadata": {},
   "source": [
    "### Step-by-step implementation guide\n",
    "1. Load the dataset from `seaborn.load_dataset()` function using **\"tips\"** argument.\n",
    "2. Map numeric-like features to integers.\n",
    "3. Split into training and test sets.\n",
    "4. Implement Euclidean distance for numeric features.\n",
    "$$d_{Euclidean}(x, y)=\\sqrt{\\sum_i(x_i-y_i)^2}$$\n",
    "5. Implement Hamming distance for categorical features.\n",
    "$$d_{Hamming}(x, y)=\\sum_i1[x_i\\neq y_i]$$\n",
    "6. Combine them with a weight $W$: \n",
    "$$d_{total}=d_{Euclidean}+W\\times d_{Hamming}$$\n",
    "7. Write your own KNN classifier using this combined distance.\n",
    "8. Evaluate accuracy for different $K$ and $W$\n",
    "> - Loop over $K \\in \\{1, 2, \\ldots, 20\\}$\n",
    "> - Loop over weights $W \\in \\{0.5, 1.0, 2.0, 3.0\\}$ (you may add more).\n",
    "> - Record accuracy for each pair $(K, W)$\n",
    "9. Plot accuracy vs. $K$ for multiple weights.\n",
    "10. Discuss how changing $W$ shifts the influence of categorical vs. numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d3e79",
   "metadata": {},
   "source": [
    "### Sample result\n",
    "\n",
    "<img src=\"images/tips-KNN-mixed-data.png\" width=\"40%\" height=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ebd3c",
   "metadata": {},
   "source": [
    "## Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fcf2f",
   "metadata": {},
   "source": [
    "- Code: Three separate scripts or notebooks, one per part.\n",
    "- plots:\n",
    "> - For parts 2 and 3, accuracy vs. $K$ plot in manual and built-in implementations.\n",
    "> - For Part 3, accuracy vs. $K$ curves for multiple $W$ values.\n",
    "- Short report:\n",
    "> - Observations: How distance choice impacts performance.\n",
    "> - Sensitivity: Effects of varying $K$ and $W$.\n",
    "> - Trade-offs: When categorical signals dominate vs. numeric signals."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
