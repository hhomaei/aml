{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62c7a45",
   "metadata": {},
   "source": [
    "<h1 align='center'>Capstone Project<br>SMS Spam Detection</h1>\n",
    "\n",
    "<center>Prepared by: <b>Ali Lakzaei</b>, Teaching Assistant</center>\n",
    "<center>Course: Applied Machine Learning</center>\n",
    "<center>Instructor: Hossein Homaei</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b26056",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project focuses on building a machine learning system to classify SMS messages as either **spam** or **ham** (legitimate messages). Students will work through the complete machine learning pipeline, from data preprocessing to model deployment and evaluation.\n",
    "\n",
    "**Dataset**: SMS Spam Collection Dataset  \n",
    "**Objective**: Develop and optimize a classification model to accurately distinguish between spam and legitimate SMS messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7486b",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "Upon completion of this project, students will be able to:\n",
    "\n",
    "- Apply data cleaning and normalization techniques to text data\n",
    "- Perform exploratory data analysis (EDA) and create meaningful visualizations\n",
    "- Select appropriate machine learning models for text classification\n",
    "- Optimize model hyperparameters and improve performance\n",
    "- Evaluate models using appropriate metrics and interpret results\n",
    "- Present technical work effectively to peers and instructors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013842e",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "The project is divided into **5 main phases**, each building upon the previous work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48d2b2",
   "metadata": {},
   "source": [
    "### **Phase 01: Data Cleaning & Normalization**\n",
    "\n",
    "**Objectives:**\n",
    "- Load and inspect the raw dataset\n",
    "- Identify and handle missing values, duplicates, and inconsistencies\n",
    "- Normalize text data (lowercasing, removing special characters, etc.)\n",
    "- Perform tokenization<b><sup>&dagger;</sup></b> and text preprocessing\n",
    "- Handle class imbalance if present\n",
    "- Split data into training, validation, and test sets\n",
    "\n",
    "\n",
    "**<sup>&dagger;</sup>Note on Tokenization:** Tokenization is the process of breaking down text into smaller units called tokens, which are typically words or subwords. For example, the sentence \"Hello world!\" would be tokenized into [\"Hello\", \"world\"]. This is a fundamental step in text preprocessing that allows machine learning models to process and analyze text data by converting raw text into a structured format that algorithms can understand. Tokenization helps in standardizing text input and is essential for feature extraction methods like Bag of Words and TF-IDF.\n",
    "\n",
    "**Deliverables:**\n",
    "- Cleaned dataset with documentation of all transformations\n",
    "- Preprocessing pipeline code\n",
    "- Summary report of data quality issues found and solutions applied\n",
    "\n",
    "**Key Tasks:**\n",
    "- [ ] Load the CSV file and examine its structure\n",
    "- [ ] Check for missing values, duplicates, and data types\n",
    "- [ ] Analyze label distribution (spam vs. ham)\n",
    "- [ ] Implement text normalization (lowercase, remove punctuation, handle special characters)\n",
    "- [ ] Remove or handle empty messages\n",
    "- [ ] Create train/validation/test splits\n",
    "- [ ] Document all preprocessing steps\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Proper handling of data quality issues\n",
    "- Appropriate text normalization techniques\n",
    "- Correct data splitting methodology\n",
    "- Clear documentation of preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2810aef",
   "metadata": {},
   "source": [
    "### **Phase 02: Data Understanding & Visualization**\n",
    "\n",
    "**Objectives:**\n",
    "- Perform exploratory data analysis (EDA)\n",
    "- Visualize class distribution and message characteristics\n",
    "- Analyze text features (length, word count, character frequency, etc.)\n",
    "- Identify patterns and insights that inform model selection\n",
    "- Create informative visualizations\n",
    "\n",
    "**Deliverables:**\n",
    "- EDA report with visualizations\n",
    "- Statistical summary of the dataset\n",
    "- Insights and observations document\n",
    "\n",
    "**Key Tasks:**\n",
    "- [ ] Analyze class distribution (spam vs. ham ratio)\n",
    "- [ ] Visualize message length distribution (characters, words) for each class\n",
    "- [ ] Analyze most common words in spam vs. ham messages\n",
    "- [ ] Create word clouds for spam and ham messages\n",
    "- [ ] Analyze frequency of special characters, numbers, URLs, etc.\n",
    "- [ ] Identify distinguishing features between spam and ham messages:\n",
    "  - [ ] Compare statistical differences (mean, median, variance) in message length, word count, and character frequencies between classes\n",
    "  - [ ] Analyze presence and frequency of specific patterns (e.g., phone numbers, currency symbols, urgency words like \"FREE\", \"WIN\", \"URGENT\")\n",
    "  - [ ] Use statistical tests (e.g., t-tests, chi-square tests) to identify features with significant differences between spam and ham\n",
    "  - [ ] Create comparative visualizations (side-by-side comparisons, overlays) to highlight differences\n",
    "- [ ] Create correlation analysis if applicable\n",
    "- [ ] Document key insights and patterns\n",
    "\n",
    "**Visualizations to Include:**\n",
    "- Bar chart: Class distribution\n",
    "- Histogram: Message length distribution (by class)\n",
    "- Box plot: Message length comparison\n",
    "- Word frequency charts (top N words per class)\n",
    "- Word clouds (spam vs. ham)\n",
    "- Analysis of special characters/numbers/URLs frequency\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Comprehensive EDA covering multiple aspects\n",
    "- Clear and informative visualizations\n",
    "- Meaningful insights derived from the analysis\n",
    "- Professional presentation of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae591e",
   "metadata": {},
   "source": [
    "### **Phase 03: Model Selection**\n",
    "\n",
    "**Objectives:**\n",
    "- Implement multiple machine learning models\n",
    "- Compare baseline models using appropriate metrics\n",
    "- Select promising models for further optimization\n",
    "- Justify model choices based on data characteristics\n",
    "\n",
    "**Deliverables:**\n",
    "- Implementation of at least 3-4 different models\n",
    "- Model comparison report\n",
    "- Selected models with justification\n",
    "\n",
    "**Key Tasks:**\n",
    "- [ ] Choose appropriate text feature extraction methods:\n",
    "  - [ ] Bag of Words (Count Vectorizer)\n",
    "  - [ ] TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "  - [ ] Word embeddings (optional: Word2Vec, GloVe, or pre-trained embeddings)\n",
    "- [ ] Implement baseline models:\n",
    "  - [ ] Naive Bayes (MultinomialNB or BernoulliNB)\n",
    "  - [ ] Logistic Regression\n",
    "  - [ ] Support Vector Machine (SVM)\n",
    "  - [ ] Random Forest (optional)\n",
    "  - [ ] Neural Network (optional, for advanced students)\n",
    "- [ ] Train each model on the training set\n",
    "- [ ] Evaluate on validation set using multiple metrics:\n",
    "  - [ ] Accuracy\n",
    "  - [ ] Precision, Recall, F1-Score\n",
    "  - [ ] Confusion Matrix\n",
    "  - [ ] ROC-AUC (if applicable)\n",
    "- [ ] Compare models and select top 2-3 for optimization\n",
    "- [ ] Document model selection rationale\n",
    "\n",
    "**Models to Implement (Minimum):**\n",
    "1. **Naive Bayes** (MultinomialNB)\n",
    "2. **Logistic Regression**\n",
    "3. **Support Vector Machine (SVM)**\n",
    "\n",
    "**Optional Models (for bonus points):**\n",
    "- Random Forest\n",
    "- Neural Network (MLP or simple deep learning)\n",
    "- Ensemble methods\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Correct implementation of feature extraction\n",
    "- Proper model training and validation\n",
    "- Appropriate metric selection and interpretation\n",
    "- Clear comparison and justification of model choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38066c",
   "metadata": {},
   "source": [
    "### **Phase 04: Model Refinement & Optimization**\n",
    "\n",
    "**Objectives:**\n",
    "- Optimize hyperparameters for selected models\n",
    "- Implement feature engineering improvements\n",
    "- Address class imbalance if needed\n",
    "- Improve model performance through iterative refinement\n",
    "\n",
    "**Deliverables:**\n",
    "- Optimized models with tuned hyperparameters\n",
    "- Comparison of before/after optimization results\n",
    "- Documentation of optimization process\n",
    "\n",
    "**Key Tasks:**\n",
    "- [ ] Perform hyperparameter tuning:\n",
    "  - [ ] Grid Search or Random Search\n",
    "  - [ ] Cross-validation (k-fold, e.g., 5-fold)\n",
    "  - [ ] Optimize for appropriate metric (F1-score recommended for imbalanced data)\n",
    "- [ ] Experiment with different feature extraction parameters:\n",
    "  - [ ] n-gram ranges (unigrams, bigrams, trigrams)\n",
    "  - [ ] Maximum features\n",
    "  - [ ] Min/max document frequency\n",
    "- [ ] Address class imbalance (if present):\n",
    "  - [ ] SMOTE or other oversampling techniques\n",
    "  - [ ] Class weights in models\n",
    "  - [ ] Undersampling (if appropriate)\n",
    "- [ ] Feature engineering:\n",
    "  - [ ] Add message length as a feature\n",
    "  - [ ] Add count of special characters, numbers, URLs\n",
    "  - [ ] Experiment with feature combinations\n",
    "- [ ] Iterate and refine based on validation results\n",
    "- [ ] Document all optimization steps and their impact\n",
    "\n",
    "**Optimization Techniques:**\n",
    "- Hyperparameter tuning (GridSearchCV/RandomSearchCV)\n",
    "- Cross-validation\n",
    "- Feature selection/engineering\n",
    "- Class imbalance handling\n",
    "- Ensemble methods (optional)\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Systematic approach to hyperparameter tuning\n",
    "- Improvement in model performance\n",
    "- Proper use of cross-validation\n",
    "- Clear documentation of optimization process\n",
    "- Justification of final model choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8fb0d5",
   "metadata": {},
   "source": [
    "### **Phase 05: Model Inference & Model Evaluation**\n",
    "\n",
    "**Objectives:**\n",
    "- Evaluate final models on the test set\n",
    "- Perform comprehensive model evaluation\n",
    "- Analyze model errors and limitations\n",
    "- Create a simple inference pipeline\n",
    "- Prepare final presentation\n",
    "\n",
    "**Deliverables:**\n",
    "- Final model evaluation report\n",
    "- Test set results with detailed metrics\n",
    "- Error analysis\n",
    "- Inference pipeline/demo\n",
    "- Final presentation\n",
    "\n",
    "**Key Tasks:**\n",
    "- [ ] Evaluate final optimized models on test set:\n",
    "  - [ ] Calculate all relevant metrics\n",
    "  - [ ] Generate confusion matrices\n",
    "  - [ ] Create ROC curves (if applicable)\n",
    "  - [ ] Calculate precision-recall curves\n",
    "- [ ] Perform error analysis:\n",
    "  - [ ] Identify common misclassification patterns\n",
    "  - [ ] Analyze false positives and false negatives\n",
    "  - [ ] Understand model limitations\n",
    "- [ ] Create inference pipeline:\n",
    "  - [ ] Function to preprocess new messages\n",
    "  - [ ] Function to make predictions\n",
    "- [ ] Test inference on sample messages\n",
    "- [ ] Prepare final presentation covering:\n",
    "  - [ ] Project overview and objectives\n",
    "  - [ ] Data exploration findings\n",
    "  - [ ] Model selection and optimization process\n",
    "  - [ ] Final results and evaluation\n",
    "  - [ ] Conclusions and future work\n",
    "\n",
    "**Evaluation Metrics (Final):**\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-Score (macro and weighted)\n",
    "- Confusion Matrix\n",
    "- ROC-AUC Score\n",
    "- Precision-Recall AUC\n",
    "- Classification Report\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Comprehensive test set evaluation\n",
    "- Deep error analysis and insights\n",
    "- Working inference pipeline\n",
    "- Professional presentation\n",
    "- Clear communication of results and methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9a6ce",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "**Dataset Source**: Download the [spam-sms-classification](https://www.kaggle.com/datasets/mariumfaheem666/spam-sms-classification-using-nlp) dataset from Kaggle. A copy of this dataset is also available [here](./spam-sms-classification-using-nlp.csv) at the Github reporitory of the course.\n",
    "\n",
    "**Structure:**\n",
    "- **v1**: Label column (ham/spam)\n",
    "- **v2**: SMS message text\n",
    "- Additional empty columns (can be ignored)\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- Binary classification problem\n",
    "- Text data requiring NLP preprocessing\n",
    "- Potential class imbalance (typical in spam detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54319e98",
   "metadata": {},
   "source": [
    "## Technical Requirements\n",
    "\n",
    "### **Programming Language & Tools:**\n",
    "- Python 3.x (recommended: 3.8+)\n",
    "- Jupyter Notebook or Python scripts for development\n",
    "- Git for version control (recommended)\n",
    "\n",
    "### **Required Libraries:**\n",
    "- Data Processing: pandas, numpy\n",
    "- Text Processing: nltk, scikit-learn, re\n",
    "- Machine Learning: scikit-learn\n",
    "- Visualization: matplotlib, seaborn, wordcloud\n",
    "- Optional: tensorflow/keras, spacy (for advanced models)\n",
    "\n",
    "### **Code Organization:**\n",
    "Organize your project with clear structure (Example):\n",
    "```\n",
    "project/\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   │   └── spam.csv\n",
    "│   └── processed/\n",
    "├── notebooks/\n",
    "│   ├── 01_data_cleaning.ipynb\n",
    "│   ├── 02_eda_visualization.ipynb\n",
    "│   ├── 03_model_selection.ipynb\n",
    "│   ├── 04_model_optimization.ipynb\n",
    "│   └── 05_evaluation_inference.ipynb\n",
    "├── src/\n",
    "│   ├── preprocessing.py\n",
    "│   ├── models.py\n",
    "│   └── inference.py\n",
    "├── results/\n",
    "│   ├── visualizations/\n",
    "│   └── model_artifacts/\n",
    "└── README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c473a0",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "**Overall Project Assessment (100 points)**\n",
    "\n",
    "| Phase | Points | Criteria |\n",
    "|-------|--------|----------|\n",
    "| Phase 01: Data Cleaning | 15 | Quality of preprocessing, handling of issues, documentation |\n",
    "| Phase 02: EDA & Visualization | 20 | Comprehensiveness, quality of visualizations, insights |\n",
    "| Phase 03: Model Selection | 20 | Correct implementation, comparison, justification |\n",
    "| Phase 04: Optimization | 20 | Systematic approach, performance improvement, documentation |\n",
    "| Phase 05: Evaluation & Presentation | 25 | Comprehensive evaluation, error analysis, presentation quality |\n",
    "\n",
    "**Bonus Points (up to 10):**\n",
    "- Advanced models (neural networks, ensembles)\n",
    "- Creative feature engineering\n",
    "- Deployment/demo application\n",
    "- Exceptional visualizations or insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeeebcc",
   "metadata": {},
   "source": [
    "## Timeline & Milestones\n",
    "**Recommended Timeline (5 weeks)**\n",
    "\n",
    "| Week | Phase | Deliverable |\n",
    "|------|-------|-------------|\n",
    "| 1 | Phase 01 | Cleaned dataset & preprocessing report |\n",
    "| 2 | Phase 02 | EDA report with visualizations |\n",
    "| 3 | Phase 03 | Model comparison report |\n",
    "| 4 | Phase 04 | Optimized models & optimization report |\n",
    "| 5 | Phase 05 | Final evaluation & presentation preparation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f76c94",
   "metadata": {},
   "source": [
    "## Submission Requirements\n",
    "\n",
    "**Code Submission:**\n",
    "- All Jupyter notebooks or Python scripts\n",
    "- Well-commented and organized code\n",
    "- Requirements.txt file with all dependencies\n",
    "- README.md with setup instructions\n",
    "\n",
    "**Documentation:**\n",
    "- Report for each phase (can be in notebooks or separate documents)\n",
    "- Final comprehensive report covering all phases\n",
    "- Presentation slides (15-20 minutes)\n",
    "\n",
    "**Final Deliverables:**\n",
    "- Code\n",
    "- Final report (PDF) covering all 5 phases\n",
    "- Presentation (PowerPoint/PDF) for final presentation\n",
    "- Working inference pipeline (demonstrable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1521f1",
   "metadata": {},
   "source": [
    "## Best Practices & Tips\n",
    "\n",
    "**Code Quality:**\n",
    "- Write clean, readable, and well-commented code\n",
    "- Use functions and classes for reusable components\n",
    "- Follow PEP 8 style guidelines\n",
    "- Use version control (Git)\n",
    "\n",
    "**Documentation:**\n",
    "- Document all assumptions and decisions\n",
    "- Explain why you chose specific approaches\n",
    "- Include references to papers/methods used\n",
    "- Keep notebooks organized with clear sections\n",
    "\n",
    "**Experimentation:**\n",
    "- Keep a log of experiments and their results\n",
    "- Save model artifacts and results\n",
    "- Use random seeds for reproducibility\n",
    "- Document hyperparameters and configurations\n",
    "\n",
    "**Presentation:**\n",
    "- Tell a story: problem → approach → results → insights\n",
    "- Use clear visualizations\n",
    "- Explain technical concepts\n",
    "- Practice your presentation timing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b70511",
   "metadata": {},
   "source": [
    "## Resources & References\n",
    "\n",
    "**Python Programming Language:**\n",
    "- Documentation: https://www.python.org/doc/\n",
    "- Style Guide: https://peps.python.org/pep-0008/\n",
    "\n",
    "**Text Preprocessing:**\n",
    "- NLTK Documentation: https://www.nltk.org/\n",
    "- Scikit-learn Text Feature Extraction: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "**Machine Learning:**\n",
    "- Scikit-learn User Guide: https://scikit-learn.org/stable/user_guide.html\n",
    "- A. Geron, *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems*, 3rd ed. O’Reilly Media, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f70bcb",
   "metadata": {},
   "source": [
    "## Questions & Support\n",
    "\n",
    "For questions or clarifications, please contact the course teaching assistant during office hours or via course communication channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b1287",
   "metadata": {},
   "source": [
    "## Academic Integrity\n",
    "\n",
    "- All work must be your own\n",
    "- Cite all sources and references\n",
    "- Collaboration on understanding concepts is encouraged, but code and reports must be individual work\n",
    "- Use of AI tools (ChatGPT, etc.) must be disclosed and properly cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf3482",
   "metadata": {},
   "source": [
    "**Good luck with your project!**\n",
    "\n",
    "*This project is designed to give you hands-on experience with the complete machine learning pipeline. Take your time, experiment, and most importantly, learn from the process.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
