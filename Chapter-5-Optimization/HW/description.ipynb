{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d814687e",
   "metadata": {},
   "source": [
    "<h1 align='center'>Homework 4<br>GradiGradient Descent for Minimizing a 3-D Function</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df9537",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "Gradient descent is a fundamental optimization technique widely used in machine learning and numerical analysis. In this project, you will implement the Gradient Descent Algorithm in Python to find the minimum of a two-variable function $f(x,y)$. You will:\n",
    "- Define a differentiable function $f(x,y)$ with a known minimum.\n",
    "- Implement gradient descent to iteratively approach the minimum.\n",
    "- Experiment with different learning rates and initial points.\n",
    "- Visualize the optimization path on 3D surface plots and 2D contour plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6f49b",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By completing this project, you will:\n",
    "- Understand the mechanics of gradient descent in multivariate optimization.\n",
    "- Learn how learning rate affects convergence.\n",
    "- Explore the impact of initial points on convergence behavior.\n",
    "- Develop skills in visualizing optimization trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23d359",
   "metadata": {},
   "source": [
    "## Implementation Guide\n",
    "### Step 1: Define the Function and Its Derivative in Python\n",
    "1. We choose a simple convex quadratic function with a clear minimum $(x,y)=(3, -2)$:\n",
    "$$f(x,y)=(x-3)^2+(y+2)^2$$\n",
    "2. Gradient:\n",
    "$$\\nabla f(x, y)=\\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)=(2(x-3), 2(y+2))$$\n",
    "### Step 2: Implement the Gradient Descent Algorithm in Python\n",
    "The update rule is:\n",
    "$$x_{k+1}=x_k-\\alpha \\cdot \\frac{\\partial f}{\\partial x}\\left(x_k, y_k\\right)$$\n",
    "$$y_{k+1}=y_k-\\alpha \\cdot \\frac{\\partial f}{\\partial y}\\left(x_k, y_k\\right)$$\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $(x_k, y_k)$ = current point\n",
    "### Step 3: Visualize the Optimization Path\n",
    "Plots will clearly show how the initial point moves step by step toward the minimum, illustrating the effect of learning rate and initialization.\n",
    "- Use these two initial points: (0,0), (5,-5)\n",
    "- For each initial point, show the `contour` plot for different learning rates: 0.05, 0.1, 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91df83",
   "metadata": {},
   "source": [
    "## Sample result\n",
    "- Case 1: Small learning rate ($\\alpha=(0.05)$)\n",
    "    - Converges slowly, requiring many iterations.\n",
    "    - Path shows small steps toward the minimum.\n",
    "- Case 2: Moderate learning rate ($\\alpha=(0.1)$)\n",
    "    - Converges efficiently to the minimum point.\n",
    "    - Smooth trajectory.\n",
    "- Case 3: Large learning rate ($\\alpha=(0.5)$)\n",
    "    - May overshoot or oscillate around the minimum.\n",
    "    - Visualization shows zig-zagging behavior.\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th><center>Initial Point</center></td>\n",
    "            <th><center>Learning Rate = 0.05</center></td>\n",
    "            <th><center>Learning Rate = 0.1</center></td>\n",
    "            <th><center>Learning Rate = 0.5</center></td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><center>(0, 0)</center></td>\n",
    "            <td><img src=\"images/GD_contour_00_0.05.png\" align=\"middle\"/></td>\n",
    "            <td><img src=\"images/GD_contour_00_0.1.png\" align=\"middle\"/></td>\n",
    "            <td><img src=\"images/GD_contour_00_0.5.png\" align=\"middle\"/></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><center>(5, -5)</center></td>\n",
    "            <td><img src=\"images/GD_contour_5-5_0.05.png\" align=\"middle\"/></td>\n",
    "            <td><img src=\"images/GD_contour_5-5_0.1.png\" align=\"middle\"/></td>\n",
    "            <td><img src=\"images/GD_contour_5-5_0.5.png\" align=\"middle\"/></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ebd3c",
   "metadata": {},
   "source": [
    "## Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fcf2f",
   "metadata": {},
   "source": [
    "- Python code implementing gradient descent to find the minimum of a two-variable function.\n",
    "- Plots showing convergence paths for different learning rates and initial points.\n",
    "- Short report discussing:\n",
    "> - Effect of learning rate on convergence.\n",
    "> - Effect of initial point choice.\n",
    "> - Observations about stability and efficiency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
